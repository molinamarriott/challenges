{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgp2WIevfdENiFajPjoy6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/molinamarriott/challenges/blob/main/WebScrappingSky.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "!pip install webdriver-manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-q9uzdcTaojk",
        "outputId": "6c577f1c-5e89-43b5-d20c-947fd22916f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [3 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 43.1 kB/88.7 kB 49%] [Connect\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 43\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 43\r                                                                               \rGet:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 27.2 kB/88.7 kB 31%] [4 InRelease 57\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 47.5 kB/88.7 kB 54%] [Waiting for he\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\r                                                                               \r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Waiting for headers]\r                                                                         \rGet:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,145 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,371 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,573 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,412 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,348 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,568 kB]\n",
            "Fetched 13.7 MB in 2s (6,264 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 95.6 MB of archives.\n",
            "After this operation, 321 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 108.0.5359.71-0ubuntu0.18.04.5 [1,159 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 108.0.5359.71-0ubuntu0.18.04.5 [83.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 108.0.5359.71-0ubuntu0.18.04.5 [5,230 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 108.0.5359.71-0ubuntu0.18.04.5 [5,594 kB]\n",
            "Fetched 95.6 MB in 4s (27.2 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_108.0.5359.71-0ubuntu0.18.04.5_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_108.0.5359.71-0ubuntu0.18.04.5_amd64.deb ...\n",
            "Unpacking chromium-browser (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_108.0.5359.71-0ubuntu0.18.04.5_all.deb ...\n",
            "Unpacking chromium-browser-l10n (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_108.0.5359.71-0ubuntu0.18.04.5_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Setting up chromium-browser (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Setting up chromium-browser-l10n (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium) (2022.12.7)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (22.2.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.7.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.14 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-3.8.5-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from webdriver-manager) (21.3)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from webdriver-manager) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from webdriver-manager) (2.25.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->webdriver-manager) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->webdriver-manager) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->webdriver-manager) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->webdriver-manager) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->webdriver-manager) (4.0.0)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-0.21.0 webdriver-manager-3.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "urls = ['https://www.skysports.com/premier-league-results/2022-23',\n",
        "        'https://www.skysports.com/premier-league-results/2021-22',\n",
        "        \"https://www.skysports.com/premier-league-results/2020-21\",\n",
        "        \"https://www.skysports.com/premier-league-results/2019-20\"]\n",
        "\n",
        "links = []\n",
        "\n",
        "for url in urls:\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  matches = soup.find_all('div', {'class': 'fixres__item'})\n",
        "\n",
        "  for match in matches:\n",
        "    link_match = str(match.find('a')[\"href\"])\n",
        "    links.append(link_match)"
      ],
      "metadata": {
        "id": "5keBeufTDf5H"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visit_url(url):\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  from selenium import webdriver\n",
        "  import pandas as pd\n",
        "\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "# Navigate to the page\n",
        "  driver.get(url)\n",
        "\n",
        "# Wait for the page to load\n",
        "  driver.implicitly_wait(1000)\n",
        "\n",
        "# Use BeautifulSoup to parse the page source\n",
        "  html = driver.page_source\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "9LtIFf0tXIiZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_variables(url_bruto):\n",
        "  url_bruto[:-6] + 'teams/' + url_bruto[-6:]\n",
        "  # Create a new instance of the Chrome driver\n",
        "  url = url_bruto[:-6] + 'stats/' + url_bruto[-6:]\n",
        "\n",
        "  soup = visit_url(url)\n",
        "\n",
        "  table = soup.find(\"div\",{\"class\":\"sdc-site-match-stats__inner\"})\n",
        "\n",
        "  stats = table.find_all(\"h5\", {'class':\"sdc-site-match-stats__name\"})\n",
        "  \n",
        "  variable_names = [\"Home\",\"Away\"]\n",
        "  values = []\n",
        "  \n",
        "  local = table.find(\"span\", {'class':\"sdc-site-match-stats__team-name\"}).get_text(strip=True)\n",
        "  away = table.find(\"span\", {'class':\"sdc-site-match-stats__team-name sdc-site-match-stats__team-name--away\"}).get_text(strip=True)\n",
        "\n",
        "  values.append(local)\n",
        "  values.append(away)\n",
        "\n",
        "  for stat in stats:\n",
        "    variable_names.append(f'Home {stat.get_text(strip=True)}')\n",
        "    variable_names.append(f'Away {stat.get_text(strip=True)}')\n",
        "\n",
        "  results = soup.find_all(\"span\", {'class':\"sdc-site-match-stats__val\"})\n",
        "  for result in results:\n",
        "    values.append(result.get_text(strip=True))\n",
        "\n",
        "  fecha = soup.find(\"time\").get_text(strip=True)\n",
        "  variable_names.append(\"Fecha\")\n",
        "  values.append(fecha)\n",
        "\n",
        "  url2 = url.replace(\"stats\", \"teams\")\n",
        "  soup = visit_url(url2)\n",
        "\n",
        "  referee = soup.find(\"dd\", {\"class\":'sdc-site-team-lineup__officials-name', 'data-officials-role':\"Referee\"}).get_text(strip=True)\n",
        "  variable_names.append(\"Referee\")\n",
        "  values.append(referee)\n",
        "\n",
        "  df = pd.DataFrame([values], columns=variable_names)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "EFU7YR5FEzQw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "for link in links:\n",
        "  if n == 1:\n",
        "    df = get_variables(link)\n",
        "  else:\n",
        "    df = df.append(get_variables(link))\n",
        "  n += 1\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "UA8Aj4QzYpA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}